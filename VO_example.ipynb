{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4c66d4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-required-packages\" data-toc-modified-id=\"Loading-the-required-packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the required packages</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Loading-the-dataset\" data-toc-modified-id=\"Loading-the-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loading the dataset</a></span></li><li><span><a href=\"#Visual-Odometry-functions\" data-toc-modified-id=\"Visual-Odometry-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Visual Odometry functions</a></span></li><li><span><a href=\"#Load-the-dataset-sequence\" data-toc-modified-id=\"Load-the-dataset-sequence-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Load the dataset sequence</a></span></li><li><span><a href=\"#Calculations\" data-toc-modified-id=\"Calculations-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculations</a></span></li><li><span><a href=\"#Visualizing-the-results\" data-toc-modified-id=\"Visualizing-the-results-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Visualizing the results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec244d3",
   "metadata": {},
   "source": [
    "# Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b27d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8568d1c",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "This is the demo for [Visual Odometry](https://www.cvlibs.net/datasets/kitti/eval_odometry.php) using the KITTI dataset. The dataset structure should be as follows:\n",
    "\n",
    "> - dataset\n",
    "    - poses\n",
    "        - 00.txt\n",
    "    - sequences\n",
    "        - 00\n",
    "            - image_0\n",
    "                - 000001.png\n",
    "            - image_1\n",
    "                - - 000001.png\n",
    "            - velodyne\n",
    "                - 000000.bin\n",
    "            - calib.txt\n",
    "            - times.txt\n",
    "            \n",
    "image_0: left images \\\n",
    "image_1: right images \\\n",
    "velodyne: velodyne laser data \\\n",
    "calib.txt: this link provides Tr [link](https://www.cvlibs.net/download.php?file=data_odometry_calib.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73a826",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f9c34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Handler():\n",
    "    def __init__(self, sequence, lidar=True):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        import cv2\n",
    "        \n",
    "        #if handler contains lidar info\n",
    "        self.lidar = lidar\n",
    "        \n",
    "        #file paths and ground truth poses\n",
    "        self.seq_dir = 'dataset/sequences/{}/'.format(sequence)\n",
    "        self.poses_dir = 'dataset/poses/{}.txt'.format(sequence)\n",
    "        poses = pd.read_csv(self.poses_dir, delimiter=' ', header=None)\n",
    "        \n",
    "        #names of files to iterate through\n",
    "        self.left_image_files = os.listdir(self.seq_dir + 'image_0')\n",
    "        self.right_image_files = os.listdir(self.seq_dir + 'image_1')\n",
    "        self.velodyne_files = os.listdir(self.seq_dir + 'velodyne')\n",
    "        \n",
    "        self.left_image_files.sort()\n",
    "        self.right_image_files.sort()\n",
    "        self.velodyne_files.sort()\n",
    "        \n",
    "        self.num_frames = len(self.left_image_files)\n",
    "        self.lidar_path = self.seq_dir + 'velodyne/'\n",
    "        \n",
    "        #calibration details for scene, P0 and P1 are Grayscale cams, P2 and P3 are RGB cams\n",
    "        calib = pd.read_csv(self.seq_dir + 'calib.txt', delimiter=' ', header=None, index_col=0)\n",
    "        self.P0 = np.array(calib.loc['P0:']).reshape((3,4))\n",
    "        self.P1 = np.array(calib.loc['P1:']).reshape((3,4))\n",
    "        self.P2 = np.array(calib.loc['P2:']).reshape((3,4))\n",
    "        self.P3 = np.array(calib.loc['P3:']).reshape((3,4))\n",
    "        # This is the transformation matrix for LIDAR\n",
    "        self.Tr = np.array(calib.loc['Tr:']).reshape((3,4))\n",
    "        \n",
    "        #times and ground truth poses\n",
    "        self.times = np.array(pd.read_csv(self.seq_dir + 'times.txt', delimiter=' ', header=None))\n",
    "        self.gt = np.zeros((len(poses), 3, 4))\n",
    "        for i in range(len(poses)):\n",
    "            self.gt[i] = np.array(poses.iloc[i]).reshape((3, 4))\n",
    "        \n",
    "        #use generators\n",
    "        self.reset_frames()\n",
    "        \n",
    "        #just for testing, store original frame to memory\n",
    "        self.first_image_left = cv2.imread(self.seq_dir + 'image_0/' \n",
    "                                           + self.left_image_files[0], 0)\n",
    "        self.first_image_right = cv2.imread(self.seq_dir + 'image_1/' \n",
    "                                           + self.right_image_files[0], 0)\n",
    "        self.second_image_left = cv2.imread(self.seq_dir + 'image_0/' \n",
    "                                           + self.left_image_files[1], 0)\n",
    "        if self.lidar:\n",
    "            self.first_pointcloud = np.fromfile(self.lidar_path + self.velodyne_files[0],\n",
    "                                                dtype=np.float32, \n",
    "                                                count=-1).reshape((-1, 4))\n",
    "        self.imheight = self.first_image_left.shape[0]\n",
    "        self.imwidth = self.first_image_left.shape[1]\n",
    "            \n",
    "            \n",
    "    def reset_frames(self):\n",
    "        # Resets all generators to the first frame of the sequence\n",
    "        self.images_left = (cv2.imread(self.seq_dir + 'image_0/' + name_left, 0)\n",
    "                            for name_left in self.left_image_files)\n",
    "        self.images_right = (cv2.imread(self.seq_dir + 'image_1/' + name_right, 0)\n",
    "                            for name_right in self.right_image_files)\n",
    "        if self.lidar:\n",
    "            self.pointclouds = (np.fromfile(self.lidar_path + velodyne_file, \n",
    "                                            dtype=np.float32, \n",
    "                                            count=-1).reshape((-1, 4))\n",
    "                                for velodyne_file in self.velodyne_files)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a66cc",
   "metadata": {},
   "source": [
    "# Visual Odometry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a35d52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate disparity map\n",
    "def compute_left_disparity_map(img_left, img_right, matcher='sgbm'):\n",
    "    \n",
    "    sad_window = 6\n",
    "    num_disparities = sad_window*16\n",
    "    block_size = 11\n",
    "    matcher_name = matcher\n",
    "    \n",
    "    if matcher_name == 'bm':\n",
    "        matcher = cv2.StereoBM_create(numDisparities=num_disparities,\n",
    "                                      blockSize=block_size\n",
    "                                     )\n",
    "        \n",
    "    elif matcher_name == 'sgbm':\n",
    "        matcher = cv2.StereoSGBM_create(numDisparities=num_disparities,\n",
    "                                        minDisparity=0,\n",
    "                                        blockSize=block_size,\n",
    "                                        P1 = 8 * 3 * sad_window ** 2,\n",
    "                                        P2 = 32 * 3 * sad_window ** 2,\n",
    "                                        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n",
    "                                       )\n",
    "\n",
    "    disparity = matcher.compute(img_left, img_right).astype(np.float32)/16\n",
    "    return disparity\n",
    "\n",
    "# to get parameters from the projection matrix\n",
    "def decompose_projection_matrix(p):\n",
    "\n",
    "    k, r, t, _, _, _, _ = cv2.decomposeProjectionMatrix(p)\n",
    "    t = (t / t[3])[:3]\n",
    "    \n",
    "    return k, r, t\n",
    "\n",
    "# calculate the depth map\n",
    "def calc_depth_map(disparity_map, k_left, t_left, t_right, rectified=True):\n",
    "    \n",
    "    # Get focal length of x axis for left camera\n",
    "    f = k_left[0][0]\n",
    "    \n",
    "    # Calculate baseline of stereo pair\n",
    "    if rectified:\n",
    "        b = t_right[0] - t_left[0] \n",
    "    else:\n",
    "        b = t_left[0] - t_right[0]\n",
    "        \n",
    "    # Avoid instability and division by zero\n",
    "    disparity_map[disparity_map == 0.0] = 0.1\n",
    "    disparity_map[disparity_map == -1.0] = 0.1\n",
    "    \n",
    "    # Make empty depth map then fill with depth\n",
    "    depth_map = np.ones(disparity_map.shape)\n",
    "    depth_map = f * b / disparity_map\n",
    "    \n",
    "    return depth_map\n",
    "\n",
    "# calculating depth using the stereo camera pair\n",
    "def stereo_2_depth(img_left, img_right, P0, P1, matcher='bm', rectified=True):\n",
    "    \n",
    "    # Compute disparity map\n",
    "    disp = compute_left_disparity_map(img_left, \n",
    "                                      img_right, \n",
    "                                      matcher=matcher)\n",
    "    # Decompose projection matrices\n",
    "    k_left, r_left, t_left = decompose_projection_matrix(P0)\n",
    "    k_right, r_right, t_right = decompose_projection_matrix(P1)\n",
    "    # Calculate depth map for left camera\n",
    "    depth = calc_depth_map(disp, k_left, t_left, t_right)\n",
    "\n",
    "    return depth\n",
    "\n",
    "# calculate depth using pointclouds obtained from LIDAR\n",
    "def pointcloud2image(pointcloud, imheight, imwidth, Tr, P0):\n",
    "    \n",
    "    pointcloud = pointcloud[pointcloud[:, 0] > 0]\n",
    "    reflectance = pointcloud[:, 3]\n",
    "    # Make pointcloud homogeneous (X, Y, Z, 1)\n",
    "    pointcloud = np.hstack([pointcloud[:, :3], np.ones(pointcloud.shape[0]).reshape((-1,1))])\n",
    "    \n",
    "    # Transform points into 3D coordinate frame of camera\n",
    "    cam_xyz = Tr.dot(pointcloud.T)\n",
    "    # Clip off negative z values\n",
    "    cam_xyz = cam_xyz[:, cam_xyz[2] > 0]\n",
    "    \n",
    "    depth = cam_xyz[2].copy()\n",
    "    \n",
    "    cam_xyz /= cam_xyz[2]    \n",
    "    cam_xyz = np.vstack([cam_xyz, np.ones(cam_xyz.shape[1])])\n",
    "    projection = P0.dot(cam_xyz)\n",
    "    pixel_coordinates = np.round(projection.T, 0)[:, :2].astype('int')\n",
    "    \n",
    "    indices = np.where((pixel_coordinates[:, 0] < imwidth)\n",
    "                       & (pixel_coordinates[:, 0] >= 0)\n",
    "                       & (pixel_coordinates[:, 1] < imheight)\n",
    "                       & (pixel_coordinates[:, 1] >= 0))\n",
    "    \n",
    "    pixel_coordinates = pixel_coordinates[indices]\n",
    "    depth = depth[indices]\n",
    "    reflectance = reflectance[indices]\n",
    "    \n",
    "    render = np.zeros((imheight, imwidth))\n",
    "    for j, (u, v) in enumerate(pixel_coordinates):\n",
    "        render[v, u] = depth[j]\n",
    "        \n",
    "    return render\n",
    "\n",
    "# extracting the keypoints and descriptors using SIFT\n",
    "def extract_features(image, mask=None):\n",
    "    \n",
    "    det = cv2.SIFT_create()      \n",
    "    kp, des = det.detectAndCompute(image, mask)\n",
    "    \n",
    "    return kp, des\n",
    "\n",
    "# matching keypoints obtained from SIFT with corresponding images\n",
    "def match_features(des1, des2, matching='BF', sort=True, k=2):\n",
    "\n",
    "    if matching == 'BF':\n",
    "        matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck=False)\n",
    "        matches = matcher.knnMatch(des1, des2, k=k)\n",
    "    elif matching == 'FLANN':\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = matcher.knnMatch(des1, des2, k=k)\n",
    "    \n",
    "    if sort:\n",
    "        matches = sorted(matches, key = lambda x:x[0].distance)\n",
    "\n",
    "    return matches\n",
    "\n",
    "# filter the keypints which do not come under the threshold    \n",
    "def filter_matches_distance(matches, dist_threshold):\n",
    "    filtered_match = []\n",
    "    for m, n in matches:\n",
    "        if m.distance <= dist_threshold*n.distance:\n",
    "            filtered_match.append(m)\n",
    "\n",
    "    return filtered_match\n",
    "\n",
    "# find the rotation matrix and the translation vector while moving from frame 1 to frame 2,\n",
    "#this gives the camera location of frame 2 wrt frame 1 which is opposite of what we want\n",
    "def estimate_motion(match, kp1, kp2, k, depth1=None, max_depth=3000):\n",
    "\n",
    "    rmat = np.eye(3)\n",
    "    tvec = np.zeros((3, 1))\n",
    "    \n",
    "    image1_points = np.float32([kp1[m.queryIdx].pt for m in match])\n",
    "    image2_points = np.float32([kp2[m.trainIdx].pt for m in match])\n",
    "\n",
    "    if depth1 is not None:\n",
    "        cx = k[0, 2]\n",
    "        cy = k[1, 2]\n",
    "        fx = k[0, 0]\n",
    "        fy = k[1, 1]\n",
    "        object_points = np.zeros((0, 3))\n",
    "        delete = []\n",
    "\n",
    "        # Extract depth information of query image at match points and build 3D positions\n",
    "        for i, (u, v) in enumerate(image1_points):\n",
    "            z = depth1[int(v), int(u)]\n",
    "            if z > max_depth:\n",
    "                delete.append(i)\n",
    "                continue\n",
    "                \n",
    "            # Use arithmetic to extract x and y (faster than using inverse of k)\n",
    "            x = z*(u-cx)/fx\n",
    "            y = z*(v-cy)/fy\n",
    "            object_points = np.vstack([object_points, np.array([x, y, z])])\n",
    "            # Equivalent math with dot product w/ inverse of k matrix, but SLOWER (see Appendix A)\n",
    "            #object_points = np.vstack([object_points, np.linalg.inv(k).dot(z*np.array([u, v, 1]))])\n",
    "\n",
    "        image1_points = np.delete(image1_points, delete, 0)\n",
    "        image2_points = np.delete(image2_points, delete, 0)\n",
    "        \n",
    "        # Use PnP algorithm with RANSAC for robustness to outliers\n",
    "        _, rvec, tvec, inliers = cv2.solvePnPRansac(object_points, image2_points, k, None)\n",
    "        rmat = cv2.Rodrigues(rvec)[0]\n",
    "    \n",
    "    else:\n",
    "        image1_points_hom = np.hstack([image1_points, np.ones(len(image1_points)).reshape(-1,1)])\n",
    "        image2_points_hom = np.hstack([image2_points, np.ones(len(image2_points)).reshape(-1,1)])\n",
    "        E = cv2.findEssentialMat(image1_points, image2_points, k)[0]\n",
    "        _, rmat, tvec, mask = cv2.recoverPose(E, image1_points, image2_points, k)\n",
    "    \n",
    "    return rmat, tvec, image1_points, image2_points\n",
    "\n",
    "# main function which loops over the images and also finds the inverse of the transformation\n",
    "# matrix calculated earlier\n",
    "def visual_odometry(handler, detector='sift', matching='BF', filter_match_distance=None,\n",
    "                    stereo_matcher='sgbm', mask=None, subset=None, plot=False):\n",
    "    # Determine if handler has lidar data\n",
    "    lidar = handler.lidar\n",
    "    \n",
    "    # Report methods being used to user\n",
    "    print('Generating disparities with Stereo{}'.format(str.upper(stereo_matcher)))\n",
    "    print('Detecting features with {} and matching with {}'.format(str.upper(detector),\n",
    "                                                                  matching))\n",
    "    if filter_match_distance is not None:\n",
    "        print('Filtering feature matches at threshold of {}*distance'.format(filter_match_distance))\n",
    "    if lidar:\n",
    "        print('Improving stereo depth estimation with lidar data')\n",
    "    if subset is not None:\n",
    "        num_frames = subset\n",
    "    else:\n",
    "        num_frames = handler.num_frames\n",
    "        \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(25, 20))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.view_init(elev=-20, azim=270)\n",
    "        xs = handler.gt[:, 0, 3]\n",
    "        ys = handler.gt[:, 1, 3]\n",
    "        zs = handler.gt[:, 2, 3]\n",
    "        ax.set_box_aspect((np.ptp(xs), np.ptp(ys), np.ptp(zs)))\n",
    "        ax.plot(xs, ys, zs, c='k', markersize=120)\n",
    "        \n",
    "    # Establish a homogeneous transformation matrix. First pose is identity\n",
    "    T_tot = np.eye(4)\n",
    "    trajectory = np.zeros((num_frames, 3, 4))\n",
    "    trajectory[0] = T_tot[:3, :]\n",
    "    imheight = handler.imheight\n",
    "    imwidth = handler.imwidth\n",
    "    \n",
    "    # Decompose left camera projection matrix to get intrinsic k matrix\n",
    "    k_left, r_left, t_left = decompose_projection_matrix(handler.P0)\n",
    "    \n",
    "    handler.reset_frames()\n",
    "    image_plus1 = next(handler.images_left)\n",
    "        \n",
    "    # Iterate through all frames of the sequence\n",
    "    for i in range(num_frames - 1):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "\n",
    "        image_left = image_plus1\n",
    "        image_plus1 = next(handler.images_left)\n",
    "        image_right = next(handler.images_right)\n",
    "\n",
    "            \n",
    "        depth = stereo_2_depth(image_left,\n",
    "                               image_right,\n",
    "                               P0=handler.P0,\n",
    "                               P1=handler.P1,\n",
    "                               matcher=stereo_matcher\n",
    "                              )\n",
    "        \n",
    "        if lidar:\n",
    "\n",
    "            pointcloud = next(handler.pointclouds)\n",
    "            \n",
    "            lidar_depth = pointcloud2image(pointcloud,\n",
    "                                           imheight=imheight,\n",
    "                                           imwidth=imwidth,\n",
    "                                           Tr=handler.Tr,\n",
    "                                           P0=handler.P0\n",
    "                                          )\n",
    "            indices = np.where(lidar_depth > 0)\n",
    "            depth[indices] = lidar_depth[indices]\n",
    "            \n",
    "        # Get keypoints and descriptors for left camera image of two sequential frames\n",
    "        kp0, des0 = extract_features(image_left)\n",
    "        kp1, des1 = extract_features(image_plus1)\n",
    "        \n",
    "        # Get matches between features detected in two subsequent frames\n",
    "        matches_unfilt = match_features(des0, \n",
    "                                        des1,\n",
    "                                        matching=matching,\n",
    "                                       )\n",
    "        #print('Number of features before filtering: ', len(matches_unfilt))\n",
    "        \n",
    "        # Filter matches if a distance threshold is provided by user\n",
    "        if filter_match_distance is not None:\n",
    "            matches = filter_matches_distance(matches_unfilt, filter_match_distance)\n",
    "        else:\n",
    "            matches = matches_unfilt\n",
    "            \n",
    "        \n",
    "        #print('Number of features after filtering: ', len(matches))\n",
    "        #print('Length of kp0:', len(kp0))\n",
    "        #print('Length of kp1:', len(kp1))\n",
    "            \n",
    "        # Estimate motion between sequential images of the left camera\n",
    "        rmat, tvec, img1_points, img2_points = estimate_motion(matches,\n",
    "                                                               kp0,\n",
    "                                                               kp1,\n",
    "                                                               k_left,\n",
    "                                                               depth\n",
    "                                                              )\n",
    "        \n",
    "        # Create a blank homogeneous transformation matrix\n",
    "        Tmat = np.eye(4)\n",
    "        Tmat[:3, :3] = rmat\n",
    "        Tmat[:3, 3] = tvec.T\n",
    "        \n",
    "        T_tot = T_tot.dot(np.linalg.inv(Tmat))\n",
    "        \n",
    "        trajectory[i+1, :, :] = T_tot[:3, :]\n",
    "        \n",
    "        end = datetime.datetime.now()\n",
    "        print('Time to compute frame {}:'.format(i+1), end-start)\n",
    "        \n",
    "        if plot:\n",
    "            xs = trajectory[:i+2, 0, 3]\n",
    "            ys = trajectory[:i+2, 1, 3]\n",
    "            zs = trajectory[:i+2, 2, 3]\n",
    "            plt.plot(xs, ys, zs, c='r', markersize=120)\n",
    "            plt.pause(1e-32)\n",
    "            \n",
    "        \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad992a8",
   "metadata": {},
   "source": [
    "# Load the dataset sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01407317",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Dataset_Handler('00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9052e5",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110a4f5",
   "metadata": {},
   "source": [
    "> Importing the matplotlib tk to visualize the plots outside this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08ac446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79908d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating disparities with StereoSGBM\n",
      "Detecting features with SIFT and matching with BF\n",
      "Filtering feature matches at threshold of 0.3*distance\n",
      "Improving stereo depth estimation with lidar data\n",
      "Time to compute frame 1: 0:00:00.219320\n",
      "Time to compute frame 2: 0:00:00.268461\n",
      "Time to compute frame 3: 0:00:00.226424\n",
      "Time to compute frame 4: 0:00:00.246406\n",
      "Time to compute frame 5: 0:00:00.239743\n",
      "Time to compute frame 6: 0:00:00.232421\n",
      "Time to compute frame 7: 0:00:00.238967\n",
      "Time to compute frame 8: 0:00:00.235196\n",
      "Time to compute frame 9: 0:00:00.225875\n",
      "Time to compute frame 10: 0:00:00.217205\n",
      "Time to compute frame 11: 0:00:00.212688\n",
      "Time to compute frame 12: 0:00:00.232611\n",
      "Time to compute frame 13: 0:00:00.218951\n",
      "Time to compute frame 14: 0:00:00.206625\n",
      "Time to compute frame 15: 0:00:00.225783\n",
      "Time to compute frame 16: 0:00:00.209125\n",
      "Time to compute frame 17: 0:00:00.227328\n",
      "Time to compute frame 18: 0:00:00.209867\n",
      "Time to compute frame 19: 0:00:00.213907\n",
      "Time to compute frame 20: 0:00:00.212560\n",
      "Time to compute frame 21: 0:00:00.216318\n",
      "Time to compute frame 22: 0:00:00.244409\n",
      "Time to compute frame 23: 0:00:00.219288\n",
      "Time to compute frame 24: 0:00:00.232177\n",
      "Time to compute frame 25: 0:00:00.216677\n",
      "Time to compute frame 26: 0:00:00.211325\n",
      "Time to compute frame 27: 0:00:00.226136\n",
      "Time to compute frame 28: 0:00:00.232932\n",
      "Time to compute frame 29: 0:00:00.215415\n",
      "Time to compute frame 30: 0:00:00.226541\n",
      "Time to compute frame 31: 0:00:00.201732\n",
      "Time to compute frame 32: 0:00:00.195750\n",
      "Time to compute frame 33: 0:00:00.230424\n",
      "Time to compute frame 34: 0:00:00.209365\n",
      "Time to compute frame 35: 0:00:00.203672\n",
      "Time to compute frame 36: 0:00:00.238070\n",
      "Time to compute frame 37: 0:00:00.198290\n",
      "Time to compute frame 38: 0:00:00.210383\n",
      "Time to compute frame 39: 0:00:00.216855\n",
      "Time to compute frame 40: 0:00:00.213777\n",
      "Time to compute frame 41: 0:00:00.190436\n",
      "Time to compute frame 42: 0:00:00.225326\n",
      "Time to compute frame 43: 0:00:00.207999\n",
      "Time to compute frame 44: 0:00:00.222607\n",
      "Time to compute frame 45: 0:00:00.190244\n",
      "Time to compute frame 46: 0:00:00.188948\n",
      "Time to compute frame 47: 0:00:00.191813\n",
      "Time to compute frame 48: 0:00:00.205130\n",
      "Time to compute frame 49: 0:00:00.218417\n",
      "Time to compute frame 50: 0:00:00.215508\n",
      "Time to compute frame 51: 0:00:00.205677\n",
      "Time to compute frame 52: 0:00:00.229269\n",
      "Time to compute frame 53: 0:00:00.234771\n",
      "Time to compute frame 54: 0:00:00.226303\n",
      "Time to compute frame 55: 0:00:00.199398\n",
      "Time to compute frame 56: 0:00:00.201060\n",
      "Time to compute frame 57: 0:00:00.203239\n",
      "Time to compute frame 58: 0:00:00.203682\n",
      "Time to compute frame 59: 0:00:00.201221\n",
      "Time to compute frame 60: 0:00:00.208260\n",
      "Time to compute frame 61: 0:00:00.200247\n",
      "Time to compute frame 62: 0:00:00.208236\n",
      "Time to compute frame 63: 0:00:00.232635\n",
      "Time to compute frame 64: 0:00:00.232004\n",
      "Time to compute frame 65: 0:00:00.219566\n",
      "Time to compute frame 66: 0:00:00.225939\n",
      "Time to compute frame 67: 0:00:00.215224\n",
      "Time to compute frame 68: 0:00:00.213913\n",
      "Time to compute frame 69: 0:00:00.197277\n",
      "Time to compute frame 70: 0:00:00.201261\n",
      "Time to compute frame 71: 0:00:00.205946\n",
      "Time to compute frame 72: 0:00:00.209534\n",
      "Time to compute frame 73: 0:00:00.202852\n",
      "Time to compute frame 74: 0:00:00.217369\n",
      "Time to compute frame 75: 0:00:00.200756\n",
      "Time to compute frame 76: 0:00:00.200786\n",
      "Time to compute frame 77: 0:00:00.232883\n",
      "Time to compute frame 78: 0:00:00.234108\n",
      "Time to compute frame 79: 0:00:00.230425\n",
      "Time to compute frame 80: 0:00:00.222961\n",
      "Time to compute frame 81: 0:00:00.221474\n",
      "Time to compute frame 82: 0:00:00.213642\n",
      "Time to compute frame 83: 0:00:00.221009\n",
      "Time to compute frame 84: 0:00:00.234532\n",
      "Time to compute frame 85: 0:00:00.235784\n",
      "Time to compute frame 86: 0:00:00.225053\n",
      "Time to compute frame 87: 0:00:00.248730\n",
      "Time to compute frame 88: 0:00:00.207569\n",
      "Time to compute frame 89: 0:00:00.252474\n",
      "Time to compute frame 90: 0:00:00.267289\n",
      "Time to compute frame 91: 0:00:00.221686\n",
      "Time to compute frame 92: 0:00:00.209464\n",
      "Time to compute frame 93: 0:00:00.235784\n",
      "Time to compute frame 94: 0:00:00.240253\n",
      "Time to compute frame 95: 0:00:00.217268\n",
      "Time to compute frame 96: 0:00:00.254967\n",
      "Time to compute frame 97: 0:00:00.283201\n",
      "Time to compute frame 98: 0:00:00.216804\n",
      "Time to compute frame 99: 0:00:00.228967\n",
      "Time to compute frame 100: 0:00:00.258658\n",
      "Time to compute frame 101: 0:00:00.280758\n",
      "Time to compute frame 102: 0:00:00.264792\n",
      "Time to compute frame 103: 0:00:00.256510\n",
      "Time to compute frame 104: 0:00:00.252897\n",
      "Time to compute frame 105: 0:00:00.203597\n",
      "Time to compute frame 106: 0:00:00.213449\n",
      "Time to compute frame 107: 0:00:00.233772\n",
      "Time to compute frame 108: 0:00:00.193836\n",
      "Time to compute frame 109: 0:00:00.217092\n",
      "Time to compute frame 110: 0:00:00.211867\n",
      "Time to compute frame 111: 0:00:00.300094\n",
      "Time to compute frame 112: 0:00:00.265066\n",
      "Time to compute frame 113: 0:00:00.267965\n",
      "Time to compute frame 114: 0:00:00.269737\n",
      "Time to compute frame 115: 0:00:00.253121\n",
      "Time to compute frame 116: 0:00:00.257530\n",
      "Time to compute frame 117: 0:00:00.267127\n",
      "Time to compute frame 118: 0:00:00.258502\n",
      "Time to compute frame 119: 0:00:00.265275\n",
      "Time to compute frame 120: 0:00:00.283767\n",
      "Time to compute frame 121: 0:00:00.262492\n",
      "Time to compute frame 122: 0:00:00.292023\n",
      "Time to compute frame 123: 0:00:00.265112\n",
      "Time to compute frame 124: 0:00:00.287089\n",
      "Time to compute frame 125: 0:00:00.274058\n",
      "Time to compute frame 126: 0:00:00.257723\n",
      "Time to compute frame 127: 0:00:00.284808\n",
      "Time to compute frame 128: 0:00:00.242324\n",
      "Time to compute frame 129: 0:00:00.258851\n",
      "Time to compute frame 130: 0:00:00.278880\n",
      "Time to compute frame 131: 0:00:00.274768\n",
      "Time to compute frame 132: 0:00:00.273478\n",
      "Time to compute frame 133: 0:00:00.305213\n",
      "Time to compute frame 134: 0:00:00.277194\n",
      "Time to compute frame 135: 0:00:00.272435\n",
      "Time to compute frame 136: 0:00:00.295680\n",
      "Time to compute frame 137: 0:00:00.258811\n",
      "Time to compute frame 138: 0:00:00.278324\n",
      "Time to compute frame 139: 0:00:00.284990\n",
      "Time to compute frame 140: 0:00:00.273444\n",
      "Time to compute frame 141: 0:00:00.266443\n",
      "Time to compute frame 142: 0:00:00.263872\n",
      "Time to compute frame 143: 0:00:00.202701\n",
      "Time to compute frame 144: 0:00:00.209323\n",
      "Time to compute frame 145: 0:00:00.200630\n",
      "Time to compute frame 146: 0:00:00.195479\n",
      "Time to compute frame 147: 0:00:00.187157\n",
      "Time to compute frame 148: 0:00:00.200902\n",
      "Time to compute frame 149: 0:00:00.223510\n"
     ]
    }
   ],
   "source": [
    "# mention the number of frames, for all the frames set it to None\n",
    "subset = 150\n",
    "trajectory = visual_odometry(handler, detector='sift', matching='BF', filter_match_distance=0.3, \n",
    "                    stereo_matcher='sgbm', mask=None, subset=subset,\n",
    "                    plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34bacd",
   "metadata": {},
   "source": [
    "# Visualizing the results\n",
    "\n",
    "<img src=\"images/VO_demo.png\" width=800 height=600 />\n",
    "\n",
    "The above demo is run for 250 frames. The black line is the ground truth and the red line is the path calculated by us. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
